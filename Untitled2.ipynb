{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ir_baseline_bm25_rm3_indri.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# %load ir_baseline_bm25_rm3_indri.py\n",
    "#############\n",
    "# Imports\n",
    "#############\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re \n",
    "import csv\n",
    "import torch\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "\n",
    "import uuid\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import bz2\n",
    "import pandas as pd\n",
    "# import dbmanager  as dbmanager\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "######################\n",
    "# Function defintions\n",
    "######################\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "def load_json(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "def remove_sc(text):\n",
    "    text = re.sub(r'<eos>',' ',text) # My method\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # My method\n",
    "    return text    \n",
    "\n",
    "\n",
    "def all_data_load(data_files_list):\n",
    "    all_data_file = './data/tvqa_all.json'\n",
    "    if not os.path.exists(all_data_file):\n",
    "        all_data = []\n",
    "        [all_data.extend(load_json(x)) for x in data_files_list]\n",
    "\n",
    "        with open(all_data_file, 'wt') as all_f:\n",
    "            json.dump(all_data, all_f, indent=4)\n",
    "        return all_data\n",
    "    else:\n",
    "        return load_json(all_data_file)\n",
    "\n",
    "def doc_to_trec(key, title):\n",
    "    trec_answer = {}\n",
    "    doc = '<DOC>\\n' +             '<DOCNO>' + str(key) + '</DOCNO>\\n' +             '<title>' + title + '</title>\\n' +             '</DOC>\\n'\n",
    "    trec_answer[str(key)] = doc\n",
    "    return trec_answer\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "def answers_to_trec(q_data):\n",
    "    trec_answers = []\n",
    "    seed = 0\n",
    "    n_answers = 5\n",
    "    ids_equivalences = {}\n",
    "    for item in q_data:\n",
    "        answer_keys = range(seed,seed+n_answers)\n",
    "        answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "        for a_item in zip(answer_keys, answers):\n",
    "            trec_answer = doc_to_trec(a_item[0], a_item[1])\n",
    "            trec_answers.append(trec_answer)\n",
    "            qa_key = str(item['qid']) + '_a' + str(a_item[0]%n_answers)\n",
    "            ids_equivalences[str(a_item[0])] = qa_key\n",
    "        seed += n_answers\n",
    "#     print('equivs len: ', len(ids_equivalences))\n",
    "    with open(all_data_ids_equiv_file, 'wt') as ids_e_f:\n",
    "        json.dump(ids_equivalences, ids_e_f, indent=4)\n",
    "    return [trec_answers, ids_equivalences]\n",
    "    \n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "#     trec_answers = {}\n",
    "#     key = 0\n",
    "#     for answer in answers:\n",
    "# #         print(key, '_', answer)\n",
    "# #         print(answer)\n",
    "#         doc = '<DOC>\\n' +             '<DOCNO>' + str(key) + '</DOCNO>\\n' +             '<TITLE>' + answer + '</TITLE>\\n' +             '</DOC>\\n'\n",
    "#         trec_answers[str(key)] = doc\n",
    "#         key += 1\n",
    "\n",
    "\n",
    "def to_trecfile(docs, filename, compression = 'yes', query=False):\n",
    "    print('Saving file: ', filename)\n",
    "#     print(len(docs))\n",
    "    \n",
    "    if compression == 'yes':\n",
    "        with gzip.open(filename,'wt') as f_out:\n",
    "            if query == True:\n",
    "                f_out.write('<parameters>\\n')\n",
    "#                 f_out.write('<index>\\n' + index_loc + '\\n</index>\\n')\n",
    "                \n",
    "            for doc in docs:\n",
    "                for key, value in doc.items():\n",
    "                    f_out.write(value)\n",
    "            if query == True:\n",
    "                f_out.write('</parameters>\\n')\n",
    "    else:\n",
    "        with open(filename,'wt') as f_out:\n",
    "            if query == True:\n",
    "                f_out.write('<parameters>\\n')\n",
    "#                 f_out.write('<index>\\n' + index_loc + '\\n</index>\\n')\n",
    "            for doc in docs:\n",
    "                for key, value in doc.items():\n",
    "                    f_out.write(value)\n",
    "            if query == True:\n",
    "                f_out.write('</parameters>\\n')\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def all_data_to_index_input(data_files_list):\n",
    "    q_data_all = all_data_load(data_files_list)\n",
    "    index_input_file = all_index_inputs + 'index_input_file_' + data_split\n",
    "    [trec_answers, ids_equiv] = answers_to_trec(q_data_all) # Use all data instead the data split\n",
    "    to_trecfile(trec_answers, index_input_file, compression = 'no')\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "def invert_ids(inverted_ids_file):\n",
    "    with open(all_data_ids_equiv_file, 'rt') as ids_equiv_f:\n",
    "        ids_equiv = json.load(ids_equiv_f)\n",
    "        inverted_id_equiv = {v: k for k, v in ids_equiv.iteritems()}\n",
    "    return inverted_id_equiv\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "def q_data_to_trec_file(q_data, filename, q_or_s):\n",
    "    indri_queries = to_indri_queries(q_data, q_or_s)\n",
    "    to_trecfile(indri_queries, filename, compression = 'no', query = True)\n",
    "    \n",
    "def to_indri_query(q_id, query, q_a_ans_aux_ids):\n",
    "    \n",
    "    q_t = {}\n",
    "#     q_text = '<query>\\n<type>indri</type>\\n' +          '<number>' + str(q_id) + '</number>\\n' +         '<text>\\n#combine( ' + remove_sc(query) + ')\\n</text>\\n'\n",
    "    q_text = '<query>\\n<type>indri</type>\\n' +          '<number>' + str(q_id) + '</number>\\n' +         '<text>\\n' + remove_sc(query) + '\\n</text>\\n'\n",
    "    for q_a_inv_id in q_a_ans_aux_ids:\n",
    "        q_text = q_text + '<workingSetDocno>' + q_a_inv_id +'</workingSetDocno>\\n'\n",
    "    q_text = q_text + '</query>\\n\\n'\n",
    "    q_t[q_id] = q_text\n",
    "    \n",
    "    \n",
    "    return q_t                    \n",
    "       \n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "def to_indri_queries(q_data, q_or_s):\n",
    "    indri_queries = []\n",
    "    inverted_ids = invert_ids(inverted_ids_file)\n",
    "\n",
    "    for item in q_data:\n",
    "#         print('id: ', item['qid'])\n",
    "        q_a_ans_aux_ids = []\n",
    "        for i in range(0,5):\n",
    "            q_id_ans = str(item['qid']) + '_a' + str(i)\n",
    "            q_a_ans_aux_ids.append(inverted_ids[q_id_ans])\n",
    "            \n",
    "            \n",
    "#             print('equiv_dict: ', q_id_ans, inverted_ids[q_id_ans])\n",
    "            \n",
    "            \n",
    "        if q_or_s == 'q':\n",
    "            indri_queries.append(to_indri_query(item['qid'], item['q'], q_a_ans_aux_ids))\n",
    "        if q_or_s == 's':\n",
    "            indri_queries.append(to_indri_query(item['qid'], item['located_sub_text'], q_a_ans_aux_ids))\n",
    "        \n",
    "        \n",
    "    return indri_queries\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "def make_folder(folder):\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "        os.makedirs(folder)\n",
    "    else:\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def build_index(index_input, index_loc):\n",
    "    if build_index_flag == 'no':\n",
    "        return\n",
    "# Build corpus index \n",
    "    if os.path.exists(index_loc):\n",
    "        shutil.rmtree(index_loc)\n",
    "        os.makedirs(index_loc)\n",
    "    else:\n",
    "        os.makedirs(index_loc) \n",
    "#     index_loc_param = '--indexPath=' + index_loc\n",
    "\n",
    "    toolkit_index = toolkit_loc + 'bin/IndriBuildIndex'\n",
    "    toolkit_parameters = [\n",
    "#                            'nohup', \n",
    "                           'sh',\n",
    "                           toolkit_index,\n",
    "                           '-collection',\n",
    "                           'TrecCollection',\n",
    "                           '-generator',\n",
    "                           'JsoupGenerator',\n",
    "                           '-threads',\n",
    "                            '16',\n",
    "                            '-input',\n",
    "                           index_input,\n",
    "                           '-index',\n",
    "                           index_loc,\n",
    "                           '-storePositions',\n",
    "                            '-keepStopwords',\n",
    "                            '-storeDocvectors',\n",
    "                            '-storeRawDocs']\n",
    "#                           ' >& ',\n",
    "#                           log_file,\n",
    "#                            '&']\n",
    "\n",
    "\n",
    "\n",
    "#     toolkit_parameters = ['ls',\n",
    "#                           index_loc]\n",
    "\n",
    "\n",
    "    print(toolkit_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(toolkit_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "#     print(out.decode(\"utf-8\"))\n",
    "#     print('Index error: ', err)\n",
    "    if err == 'None':\n",
    "        return 'Ok'\n",
    "\n",
    "def retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, hits, b=0.2, k=0.8, N=10, M=10, Lambda=0.5):\n",
    "    #     print(q_topics_file)\n",
    "    #print(hits)\n",
    "    \n",
    "    index_param = '-index='+ os.path.realpath(index_loc)\n",
    "#     print(index_param)\n",
    "    count = '-count=' + str(hits)\n",
    "    baseline = '-baseline=okapi,b:' + str(b) + ',k1:' + str(k)\n",
    "    N_val = '-fbDocs=' + str(N)\n",
    "    M_val = '-fbTerms=' + str(M)\n",
    "    threads_val = '-threads=' + str(12)\n",
    "    Lambda_val = '-fbOrigWeight=' + str(Lambda)\n",
    "    q_topics_file = os.path.realpath(q_topics_file)\n",
    "    toolkit_search = toolkit_loc + 'bin/IndriRunQuery'\n",
    "    command = [ \n",
    "#                'sh',\n",
    "               toolkit_search,\n",
    "               q_topics_file,\n",
    "               index_param,\n",
    "               count,\n",
    "               '-trecFormat=true',\n",
    "               baseline,\n",
    "                N_val,\n",
    "                M_val,\n",
    "                Lambda_val,\n",
    "                threads_val\n",
    "               ]\n",
    "    print(command)\n",
    "    #     command = command.encode('utf-8')\n",
    "    toolkit_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = toolkit_exec.communicate()\n",
    "    print('Opening file: ', retrieved_docs_file)\n",
    "    with open(retrieved_docs_file, 'wt') as run_f:\n",
    "        run_f.write(out.decode(\"utf-8\"))\n",
    "#     print(out.decode(\"utf-8\"))\n",
    "    print('Index error: ', err)\n",
    "    if err == 'None':\n",
    "        \n",
    "        return 'Ok'\n",
    "\n",
    "    \n",
    "def generate_preds_file(retrieved_docs_file):\n",
    "\n",
    "    with open(retrieved_docs_file, 'rt') as f_in:\n",
    "        try: \n",
    "            for doc in f_in:\n",
    "    #                 print(doc)\n",
    "                q_id = doc.split(' ')[0]\n",
    "    #                 print(doc.split(' ')[2])\n",
    "                pred_ans_id = doc.split(' ')[2]\n",
    "\n",
    "            return pred_ans_id\n",
    "        except:\n",
    "            pred_ans_id = int(6) # When BM25+RM3 does not find any document, return an answer index outside the valid answer range\n",
    "            return pred_ans_id\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(predicted_answers, gold_answers):\n",
    "    \n",
    "#     print('preds: ', predicted_answers)\n",
    "#     print('golds: ', gold_answers)\n",
    "    \n",
    "    preds = np.asarray(predicted_answers)\n",
    "    targets = np.asarray(gold_answers)\n",
    "    acc = sum(preds == targets) / float(len(preds))\n",
    "    return acc\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "def load_predictions(retrieved_docs_file):\n",
    "    preds = []\n",
    "    golds = []\n",
    "    \n",
    "    with open(all_data_ids_equiv_file, 'rt') as ids_equiv_f:\n",
    "        ids_equiv = json.load(ids_equiv_f)\n",
    "    \n",
    "    with open(retrieved_docs_file, 'rt') as r_file:\n",
    "        \n",
    "        ret_qs = {}\n",
    "        for doc in r_file:\n",
    "            qid_ori = doc.split(' ')[0]\n",
    "            aux_id = doc.split(' ')[2]\n",
    "            ret_qs[qid_ori] = aux_id\n",
    "        \n",
    "        for qid, gold in gold_answers_dict.items():\n",
    "#             print('gold_qid: ', qid)\n",
    "#             print('gold_value: ', gold)\n",
    "            golds.append(int(gold))\n",
    "    \n",
    "            if str(qid) in ret_qs.keys():\n",
    "                aux_id = ret_qs[qid]\n",
    "\n",
    "                qid_aux = ids_equiv[aux_id].split('_')[0]\n",
    "                qa_key = ids_equiv[aux_id]\n",
    "    #             print('qid, aux, gold, : ', qid, aux_id, gold, qid_aux)\n",
    "                if str(qid) == str(qid_aux):\n",
    "                    pred = int(qa_key.split('_')[1].lstrip('a'))\n",
    "                else:\n",
    "                    pred = int(6) # Found wrong doc qid answer\n",
    "            else:\n",
    "                print('Qid not in retrieved: ', qid)\n",
    "                \n",
    "                pred = int(7) # Answer not found in index\n",
    "    \n",
    "    \n",
    "#             try:\n",
    "#                 aux_id = ret_qs[qid]\n",
    "#                 print('qid, aux, gold, : ', qid, aux_id, gold)\n",
    "#                 qid_aux = ids_equiv[aux_id].split('_')[0]\n",
    "#                 if str(qid) == str(qid_aux):\n",
    "#                     pred = int(qa_key.split('_')[1].lstrip('a'))\n",
    "#                 else:\n",
    "#                     pred = int(6) # Found wrong doc qid answer\n",
    "#             except: \n",
    "#                 pred = int(7) # Answer not found in index\n",
    "            preds.append(pred) \n",
    "                \n",
    "#     print('len preds: ', len(preds))\n",
    "#     print('len golds: ', len(gold_answers_dict))\n",
    "    \n",
    "    return [preds, golds]\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "# def evaluate_params(params):\n",
    "#     b = params[0]\n",
    "#     k = params[1]\n",
    "#     N = params[2]\n",
    "#     M = params[3]\n",
    "#     Lambda = params[4]\n",
    "    \n",
    "#     params_suffix = 'b' + str(b) + 'k' + str(k) + 'N' + str(N) + 'M' + str(M) + 'Lambda' + str(Lambda) + 'n_rand_iter' + str(n_rand_iter) + 'hits' + str(hits)\n",
    "#     retrieved_docs_file = all_retrieved_files + 'run_bm25_rm3_preds_' + 'tvqa' + '_' + data_split + '_' + params_suffix + '.txt'\n",
    "    \n",
    "# ####    retrieve_docs(q_topics_file, retrieved_docs_file, all_index_dir, hits, b, k, N, M, Lambda)\n",
    "#     retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, hits, b, k, N, M, Lambda)\n",
    "    \n",
    "#     [pred_answers, gold_answers] = load_predictions(retrieved_docs_file)\n",
    "    \n",
    "#     temp_id = uuid.uuid4().hex\n",
    "#     temp_dir = all_retrieved_files + temp_id + '/'\n",
    "    \n",
    "#     if not os.path.exists(temp_dir):\n",
    "#         os.makedirs(temp_dir)\n",
    "    \n",
    "# #     pred_answers = []\n",
    "# #     gold_answers = []\n",
    "# #     for item in questions_data:\n",
    "# #         answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "# #         if model_type == 'qa':\n",
    "# #             q_data = [item['qid'], item['q'], answers]\n",
    "# #         elif model_type == 'sa':\n",
    "# #             q_data = [item['qid'], item['q'], answers]\n",
    "# #         elif model_type == 'retrieval':\n",
    "# #             print('Retrieval model to be built...')\n",
    "# # #             q_data = [item['qid'], item['q'], answers]\n",
    "\n",
    "# #         predicted_answer_id = baseline_compute(q_data, temp_dir, b,k,N,M,Lambda)\n",
    "# #         pred_answers.append(int(predicted_answer_id))\n",
    "# #         gold_answers.append(int(item['answer_idx']))\n",
    "    \n",
    "#     acc = evaluate(pred_answers, gold_answers)\n",
    "#     results = [\n",
    "#         b,\n",
    "#         k,\n",
    "#         N,\n",
    "#         M,\n",
    "#         Lambda,\n",
    "#         float(acc)\n",
    "#     ]\n",
    "#     temp_file = all_retrieved_files + temp_id + str(int(time.time())) + '.txt'\n",
    "#     string_print = 'task: ' + temp_id + ' finished!\\n'\n",
    "#     with open(temp_file, 'wt') as task_file:\n",
    "#         task_file.write(string_print)\n",
    "#         task_file.write(json.dumps(results, indent=4))\n",
    "    \n",
    "# #     shutil.rmtree(temp_dir)    \n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "def evaluate_params(retrieved_docs_file):\n",
    "#     b = params[0]\n",
    "#     k = params[1]\n",
    "#     N = params[2]\n",
    "#     M = params[3]\n",
    "#     Lambda = params[4]\n",
    "    \n",
    "#     params_suffix = 'b' + str(b) + 'k' + str(k) + 'N' + str(N) + 'M' + str(M) + 'Lambda' + str(Lambda) + 'n_rand_iter' + str(n_rand_iter) + 'hits' + str(hits)\n",
    "#     retrieved_docs_file = all_retrieved_files + 'run_bm25_rm3_preds_' + 'tvqa' + '_' + data_split + '_' + params_suffix + '.txt'\n",
    "    \n",
    "####    retrieve_docs(q_topics_file, retrieved_docs_file, all_index_dir, hits, b, k, N, M, Lambda)\n",
    "   # retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, hits, b, k, N, M, Lambda)\n",
    "    \n",
    "    [pred_answers, gold_answers] = load_predictions(retrieved_docs_file)\n",
    "    \n",
    "    temp_id = uuid.uuid4().hex\n",
    "    temp_dir = all_retrieved_files + temp_id + '/'\n",
    "    \n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    \n",
    "#     pred_answers = []\n",
    "#     gold_answers = []\n",
    "#     for item in questions_data:\n",
    "#         answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "#         if model_type == 'qa':\n",
    "#             q_data = [item['qid'], item['q'], answers]\n",
    "#         elif model_type == 'sa':\n",
    "#             q_data = [item['qid'], item['q'], answers]\n",
    "#         elif model_type == 'retrieval':\n",
    "#             print('Retrieval model to be built...')\n",
    "# #             q_data = [item['qid'], item['q'], answers]\n",
    "\n",
    "#         predicted_answer_id = baseline_compute(q_data, temp_dir, b,k,N,M,Lambda)\n",
    "#         pred_answers.append(int(predicted_answer_id))\n",
    "#         gold_answers.append(int(item['answer_idx']))\n",
    "    \n",
    "    b = re.findall(\"b[0-9].[0-9]*\", retrieved_docs_file)[0].strip('b')\n",
    "#     print(b)\n",
    "    k = re.findall(\"k[0-9].[0-9]*\", retrieved_docs_file)[0].strip('k')\n",
    "#     print(k)\n",
    "    N = re.findall(\"N[0-9]*\", retrieved_docs_file)[0].strip('N')\n",
    "#     print(N)\n",
    "    M = re.findall(\"M[0-9]*\", retrieved_docs_file)[0].strip('M')\n",
    "#     print(M)\n",
    "    Lambda = re.findall(\"Lambda[0-9].[0-9]\", retrieved_docs_file)[0].strip('Lambda')\n",
    "#     print(Lambda)\n",
    "    \n",
    "    acc = evaluate(pred_answers, gold_answers)\n",
    "    results = [\n",
    "        b,\n",
    "        k,\n",
    "        N,\n",
    "        M,\n",
    "        Lambda,\n",
    "        float(acc)\n",
    "    ]\n",
    "    temp_file = all_retrieved_files + temp_id + str(int(time.time())) + '.txt'\n",
    "    string_print = 'task: ' + temp_id + ' finished!\\n'\n",
    "    with open(temp_file, 'wt') as task_file:\n",
    "        task_file.write(string_print)\n",
    "        task_file.write(json.dumps(results, indent=4))\n",
    "    \n",
    "#     shutil.rmtree(temp_dir) \n",
    "#     print(retrieved_docs_file)\n",
    "#     print(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "def get_random_params(hyper_params, num_iter):\n",
    "    random_h_params_list = []\n",
    "    while len(random_h_params_list) < num_iter:\n",
    "        random_h_params_set = []\n",
    "        for h_param_list in hyper_params:\n",
    "            sampled_h_param = random.sample(list(h_param_list), k=1)\n",
    "#             print(type(sampled_h_param[0]))\n",
    "#             print(sampled_h_param[0])\n",
    "            random_h_params_set.append(round(sampled_h_param[0], 3))\n",
    "        if not random_h_params_set in random_h_params_list:\n",
    "            random_h_params_list.append(random_h_params_set)\n",
    "#             print('Non repeated')\n",
    "        else:\n",
    "            print('repeated')\n",
    "    return random_h_params_list\n",
    "\n",
    "\n",
    "# def find_best_dev_model(best_model_params_file, n_rand_iter, pool_size):\n",
    "# #     random_search = 'yes'\n",
    "#     make_folder(all_retrieved_files)\n",
    "#     if random_search == 'yes':\n",
    "#         ## Heavy random search\n",
    "#         brange = np.arange(0.1,1,0.05)\n",
    "#         krange = np.arange(0.1,4,0.1)\n",
    "#         N_range = np.arange(5,500,1) # num of docs\n",
    "#         M_range = np.arange(5,500,1) # num of terms\n",
    "#         lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "#         ## Light random search\n",
    "# #         brange = [0.2]\n",
    "# #         krange = [0.8]\n",
    "# #         N_range = np.arange(1,50,2)\n",
    "# #         M_range = np.arange(1,50,2)\n",
    "# #         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "#         h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "#         params = get_random_params(h_param_ranges, n_rand_iter)\n",
    "\n",
    "#     else:\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = [11]\n",
    "#         M_range = [10]\n",
    "#         lamb_range = [0.5]\n",
    "       \n",
    "#         params = [[round(b,3), round(k,3), round(N,3), round(M,3), round(Lambda,3)] \n",
    "#                   for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "    \n",
    "# #     print(len(params))\n",
    "#     pool = multiprocessing.Pool(processes=pool_size,\n",
    "#                                 initializer=start_process,\n",
    "#                                 )\n",
    "\n",
    "# #     pool_outputs = pool.map(baseline_computing, params)\n",
    "    \n",
    "\n",
    "#     pool_outputs = pool.map_async(evaluate_params, params)\n",
    "# #     print(pool_outputs.get())\n",
    "#     ###\n",
    "\n",
    "    \n",
    "#     ##\n",
    "    \n",
    "    \n",
    "#     pool.close() # no more tasks\n",
    "#     while (True):\n",
    "#         if (pool_outputs.ready()): break\n",
    "#         remaining = pool_outputs._number_left\n",
    "# #         remaining2 = remaining1\n",
    "# #         remaining1 = pool_outputs._number_left\n",
    "#         if remaining%10 == 0:\n",
    "#             print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "#             time.sleep(2)\n",
    "        \n",
    "      \n",
    "#     pool.join()  # wrap up current tasks\n",
    "#     pool_outputs.get()\n",
    "#     params_file = best_model_dir + 'tvqa' + '_' + 'bm25_rm3_dev_hparams.pickle'\n",
    "#     pickle.dump(pool_outputs.get(), open(params_file, \"wb\" ) )\n",
    "#     print('Total parameters tested: ' + str(len(pool_outputs.get())))\n",
    "#     best_model_params = max(pool_outputs.get(), key=lambda x: x[5])\n",
    "    \n",
    "#     best_model_dict = {\n",
    "#         'b': best_model_params[0],\n",
    "#         'k': best_model_params[1],\n",
    "#         'N': best_model_params[2],\n",
    "#         'M': best_model_params[3],\n",
    "#         'Lambda': best_model_params[4],\n",
    "#         'n_rand_iter': n_rand_iter,\n",
    "#         'hits': hits,\n",
    "#         'auc_score': best_model_params[5],\n",
    "#     }\n",
    "#     best_model_dict = {k:str(v) for k, v in best_model_dict.items()} # everything to string\n",
    "    \n",
    "#     print(best_model_dict)\n",
    "#     with open(best_model_params_file, 'wt') as best_model_f:\n",
    "#         json.dump(best_model_dict, best_model_f)\n",
    "\n",
    "def find_best_dev_model(best_model_params_file, n_rand_iter, pool_size):\n",
    "#     random_search = 'yes'\n",
    "#     make_folder(all_retrieved_files)\n",
    "    if random_search == 'yes':\n",
    "        ## Heavy random search\n",
    "        brange = np.arange(0.1,1,0.05)\n",
    "        krange = np.arange(0.1,4,0.1)\n",
    "        N_range = np.arange(5,500,1) # num of docs\n",
    "        M_range = np.arange(5,500,1) # num of terms\n",
    "        lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "        ## Light random search\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = np.arange(1,50,2)\n",
    "#         M_range = np.arange(1,50,2)\n",
    "#         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "        h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "       # params = get_random_params(h_param_ranges, n_rand_iter)\n",
    "\n",
    "    else:\n",
    "        brange = [0.2]\n",
    "        krange = [0.8]\n",
    "        N_range = [11]\n",
    "        M_range = [10]\n",
    "        lamb_range = [0.5]\n",
    "       \n",
    "        #params = [[round(b,3), round(k,3), round(N,3), round(M,3), round(Lambda,3)] \n",
    "#                   for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(len(params))\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "#     pool_outputs = pool.map(baseline_computing, params)\n",
    "\n",
    "    r_dir = workdir + 'retrieved_files_dev/'\n",
    "    r_files = [os.path.join(root, name)\n",
    "                for root, dirs, files in os.walk(r_dir)\n",
    "                for name in files\n",
    "                if all(y in name for y in ['run_', '.txt'])]\n",
    "\n",
    "#     print(r_files)\n",
    "    \n",
    "    pool_outputs = pool.map_async(evaluate_params, r_files)\n",
    "    print(pool_outputs.get())\n",
    "    ###\n",
    "\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    \n",
    "    pool.close() # no more tasks\n",
    "    while (True):\n",
    "        if (pool_outputs.ready()): break\n",
    "        remaining = pool_outputs._number_left\n",
    "#         remaining2 = remaining1\n",
    "#         remaining1 = pool_outputs._number_left\n",
    "        if remaining%10 == 0:\n",
    "            print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "      \n",
    "    pool.join()  # wrap up current tasks\n",
    "    pool_outputs.get()\n",
    "    params_file = best_model_dir + 'tvqa' + '_' + 'bm25_rm3_dev_hparams.pickle'\n",
    "    pickle.dump(pool_outputs.get(), open(params_file, \"wb\" ) )\n",
    "    print('Total parameters tested: ' + str(len(pool_outputs.get())))\n",
    "    best_model_params = max(pool_outputs.get(), key=lambda x: x[5])\n",
    "    \n",
    "    best_model_dict = {\n",
    "        'b': best_model_params[0],\n",
    "        'k': best_model_params[1],\n",
    "        'N': best_model_params[2],\n",
    "        'M': best_model_params[3],\n",
    "        'Lambda': best_model_params[4],\n",
    "        'n_rand_iter': n_rand_iter,\n",
    "        'hits': hits,\n",
    "        'auc_score': best_model_params[5],\n",
    "    }\n",
    "    best_model_dict = {k:str(v) for k, v in best_model_dict.items()} # everything to string\n",
    "    \n",
    "    print(best_model_dict)\n",
    "    with open(best_model_params_file, 'wt') as best_model_f:\n",
    "        json.dump(best_model_dict, best_model_f)\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "# Main defition\n",
    "#################\n",
    "\n",
    "\n",
    "# In[38]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random_search = 'yes'\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    index_loc = '/home/francisco/msc_project/not-a-punching-bag/reproduction/TVQA/workdir6/indri_index'\n",
    "    \n",
    "    print('index_loc: ',index_loc)\n",
    "\n",
    "    train_file = './data/tvqa_train_processed.json'\n",
    "    val_file = './data/tvqa_val_processed.json'\n",
    "    test_file = './data/tvqa_test_public_processed.json'\n",
    "    \n",
    "#     data_files_list = [train_file, val_file, test_file]\n",
    "    data_files_list = [train_file]\n",
    "    \n",
    "#     input_file = sys.argv[1] # './data/tvqa_new_dev_processed.json'\n",
    "#     data_split = str(sys.argv[2]) # 'dev'\n",
    "#     n_rand_iter = int(sys.argv[3]) # 5000\n",
    "#     pool_size = int(sys.argv[4]) # 20\n",
    "#     model_type = str(sys.argv[5])# q(uestion) / s(ubtitle)\n",
    "    \n",
    "    \n",
    "    input_file ='./data/tvqa_new_dev_processed.json'\n",
    "    data_split = 'dev'\n",
    "    n_rand_iter = 1\n",
    "    pool_size = 1\n",
    "    model_type = 's'\n",
    "    \n",
    "    \n",
    "    build_index_flag = 'yes'\n",
    "    \n",
    "    workdir = './workdir6/'\n",
    "    \n",
    "    all_data_ids_equiv_file = workdir + 'all_data_ids_equiv.json'\n",
    "    inverted_ids_file = workdir + 'ids_equiv.json' \n",
    "    \n",
    "    if not os.path.exists(workdir):\n",
    "        os.makedirs(workdir)\n",
    "    \n",
    "    hits = 1\n",
    "    toolkit_loc = '../indri/'\n",
    "    \n",
    "    \n",
    "    q_data_all = all_data_load(data_files_list)\n",
    "    \n",
    "    q_data = load_json(input_file)\n",
    "    \n",
    "    \n",
    "    #########\n",
    "    \n",
    "#    q_data = q_data[0:100]\n",
    "    \n",
    "    #########\n",
    "    \n",
    "    \n",
    "    all_index_dir = workdir + 'index_dirs_' + data_split + '/'\n",
    "    all_index_inputs = workdir + 'index_inputs_/' \n",
    "    all_query_files = workdir + 'query_files_' + data_split + '/'\n",
    "    all_sub_files = workdir + 'sub_files_' + data_split + '/'\n",
    "#     all_retrieved_files = workdir + 'retrieved_files_' + data_split + '/'\n",
    "#     best_model_dir = workdir + 'best_ir_model/'\n",
    "    \n",
    "#     make_folder(all_index_inputs)\n",
    "#     make_folder(all_query_files)\n",
    "#     make_folder(all_sub_files)\n",
    "#     make_folder(all_index_dir)\n",
    "#     make_folder(best_model_dir)\n",
    "    \n",
    "    # Convert answers to one index_inpu_trec_doc_file\n",
    "    # Save the index numbering equivalences (newid = queryid_a0, queryid_a1, ...)\n",
    "    \n",
    "    index_input_file = all_index_inputs + 'index_input_file_' + data_split\n",
    "    [trec_answers, ids_equiv] = answers_to_trec(q_data_all) # Use all data instead the data split\n",
    "    to_trecfile(trec_answers, index_input_file, compression = 'no')\n",
    "    all_data_to_index_input(data_files_list)\n",
    "    \n",
    "    \n",
    "    # Convert all questions / subtitles to one trec topics file \n",
    "    \n",
    "    query_topics_file = all_query_files + 'query_indri_file_' + data_split\n",
    "#     q_data_to_trec_file(q_data, query_topics_file, q_or_s = 'q')\n",
    "    \n",
    "    subtitles_topics_file = all_query_files + 'subtitle_indri_query_file_' + data_split\n",
    "    q_data_to_trec_file(q_data, subtitles_topics_file, q_or_s = 's')\n",
    "    \n",
    "    # Build index, single process\n",
    "    ##\n",
    "    build_index(all_index_inputs, all_index_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724575"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trec_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18 11:34:12.165134\n",
      "index_loc:  /home/francisco/msc_project/not-a-punching-bag/reproduction/TVQA/workdir6/indri_index\n",
      "Saving file:  ./workdir6/index_inputs_dev/index_input_file_dev\n"
     ]
    }
   ],
   "source": [
    "#     # Retrieve 1 (most relevant) answer/doc per question - multiprocessing 5000\n",
    "    \n",
    "# #     if model_type == 'q':\n",
    "# #         print('Model type: question-answer')\n",
    "# #         q_topics_file = query_topics_file\n",
    "# #     elif model_type == 's':\n",
    "# #         print('Model type: subtitle-answer')\n",
    "# #         q_topics_file = subtitles_topics_file\n",
    "        \n",
    "    \n",
    "# #     # Global gold answers list\n",
    "# #     gold_answers_dict = {}\n",
    "# #     for q in  q_data:\n",
    "# #         gold_answers_dict[str(q['qid'])] = q['answer_idx']\n",
    "\n",
    "# # #     print('type gold: ', type(gold_answers_dict.keys()[0]))\n",
    "    \n",
    "# #     # Pick best model according to accuracy\n",
    "# # #     if data_split == 'dev':\n",
    "# # #         find_best_model()\n",
    "# #     best_model_params_file = best_model_dir + 'tvqa' + '_bm25_rm3_best_model_dev.json'\n",
    "    \n",
    "# # #     params = [1,1,1,1,1]\n",
    "# # #     evaluate_params(params)\n",
    "# # ###     retrieved_docs_file = './workdir6/retrieved_files_dev/run_bm25_rm3_preds_tvqa_dev_b0.45k1.5N275.0M134.0Lambda0.9n_rand_iter500hits1.txt'\n",
    "# #   ##  print(evaluate_params(retrieved_docs_file))\n",
    "# #     find_best_dev_model(best_model_params_file, n_rand_iter, pool_size)\n",
    "    \n",
    "# # #     print(q_data[0])\n",
    "    \n",
    "# #     # Test on test set\n",
    "# # #     if data_split == 'test':\n",
    "# # #         evaluate_model()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
