{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ir_baseline_bm25_rm3.py\n",
    "\n",
    "\n",
    "# ## TVQA - building index separately\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re \n",
    "import csv\n",
    "import torch\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "\n",
    "import uuid\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import bz2\n",
    "import pandas as pd\n",
    "# import dbmanager  as dbmanager\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "# nltk.download('punkt') # for english sentences tokenization\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "## Load non processed data\n",
    "# file = './data/tvqa_qa_release/tvqa_train.jsonl'\n",
    "# with open(file, 'r') as f:\n",
    "#     lines = []\n",
    "#     for l in f.readlines():\n",
    "#         loaded_l = json.loads(l.strip(\"\\n\"))\n",
    "#         lines.append(loaded_l)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_to_trec(answers):\n",
    "    trec_answers = {}\n",
    "    key = 0\n",
    "    for answer in answers:\n",
    "#         print(key, '_', answer)\n",
    "#         print(answer)\n",
    "        doc = '<DOC>\\n' +             '<DOCNO>' + str(key) + '</DOCNO>\\n' +             '<TITLE>' + answer + '</TITLE>\\n' +             '</DOC>\\n'\n",
    "        trec_answers[str(key)] = doc\n",
    "        key += 1\n",
    "    return trec_answers\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# def generate_index_dir():\n",
    "\n",
    "# try:\n",
    "#                 golden_file = sys.argv[1]\n",
    "#                 predictions_file = sys.argv[2]\n",
    "#         except:\n",
    "#                 sys.exit(\"Provide golden and predictions files.\")\n",
    "        \n",
    "#         try:\n",
    "#                 system_name = sys.argv[3]\n",
    "#         except :\n",
    "#                 try:\n",
    "#                         system_name = predictions_file.split('/')[-1]\n",
    "#                 except:\n",
    "#                         system_name = predictions_file\n",
    "\n",
    "#         with open(golden_file, 'r') as f:\n",
    "#                 golden_data = json.load(f)\n",
    "\n",
    "#         with open(predictions_file, 'r') as f:\n",
    "#                 predictions_data = json.load(f)\n",
    "\n",
    "#         temp_dir = uuid.uuid4().hex\n",
    "#         qrels_temp_file = '{0}/{1}'.format(temp_dir, 'qrels.txt')\n",
    "#         qret_temp_file = '{0}/{1}'.format(temp_dir, 'qret.txt')\n",
    "\n",
    "#         try:\n",
    "#                 if not os.path.exists(temp_dir):\n",
    "#                         os.makedirs(temp_dir)\n",
    "#                 else:\n",
    "#                         sys.exit(\"Possible uuid collision\")\n",
    "\n",
    "#                 format_bioasq2treceval_qrels(golden_data, qrels_temp_file)\n",
    "#                 format_bioasq2treceval_qret(predictions_data, system_name, qret_temp_file)\n",
    "\n",
    "#                 trec_evaluate(qrels_temp_file, qret_temp_file)\n",
    "#         finally:\n",
    "#                 os.remove(qrels_temp_file)\n",
    "#                 os.remove(qret_temp_file)\n",
    "#                 os.rmdir(temp_dir)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trecfile(docs, filename, compression = 'yes'):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    if compression == 'yes':\n",
    "        with gzip.open(filename,'wt') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)\n",
    "    else:\n",
    "        with open(filename,'wt') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(index_input, index_loc):\n",
    "    if build_index_flag == 'no':\n",
    "        return\n",
    "# Build corpus index \n",
    "    if os.path.exists(index_loc):\n",
    "        shutil.rmtree(index_loc)\n",
    "        os.makedirs(index_loc)\n",
    "    else:\n",
    "        os.makedirs(index_loc) \n",
    "#     index_loc_param = '--indexPath=' + index_loc\n",
    "\n",
    "    anserini_index = anserini_loc + 'target/appassembler/bin/IndexCollection'\n",
    "    anserini_parameters = [\n",
    "#                            'nohup', \n",
    "                           'sh',\n",
    "                           anserini_index,\n",
    "                           '-collection',\n",
    "                           'TrecCollection',\n",
    "                           '-generator',\n",
    "                           'JsoupGenerator',\n",
    "                           '-threads',\n",
    "                            '1',\n",
    "                            '-input',\n",
    "                           index_input,\n",
    "                           '-index',\n",
    "                           index_loc,\n",
    "                           '-storePositions',\n",
    "                            '-keepStopwords',\n",
    "                            '-storeDocvectors',\n",
    "                            '-storeRawDocs']\n",
    "#                           ' >& ',\n",
    "#                           log_file,\n",
    "#                            '&']\n",
    "\n",
    "\n",
    "\n",
    "#     anserini_parameters = ['ls',\n",
    "#                           index_loc]\n",
    "\n",
    "\n",
    "#     print(anserini_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(anserini_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "#     print(out.decode(\"utf-8\"))\n",
    "#     print('Index error: ', err)\n",
    "    if err == 'None':\n",
    "        return 'Ok'\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_indexes(q_data):\n",
    "    query_id = q_data['qid']\n",
    "    query = q_data['q']\n",
    "    sub = q_data['located_sub_text']\n",
    "    answers = [q_data['a' + str(i)] for i in range(0,5)]\n",
    "    \n",
    "#     print(query_id)\n",
    "    \n",
    "    index_input_dir = all_index_inputs + str(query_id) + '_input/' \n",
    "    index_input_file = index_input_dir + str(query_id) + '_trec_input_file'\n",
    "    \n",
    "    index_location = all_index_dir + str(query_id) + '_index' \n",
    "    \n",
    "    trec_query_file = all_query_files + str(query_id) + 'trec_query_file'\n",
    "    \n",
    "    trec_sub_file = all_sub_files + str(query_id) +  'trec_sub_file'\n",
    "    \n",
    "    \n",
    "    if os.path.exists(index_input_dir):\n",
    "        shutil.rmtree(index_input_dir)\n",
    "        os.makedirs(index_input_dir)\n",
    "    else:\n",
    "        os.makedirs(index_input_dir)\n",
    "        \n",
    "    if os.path.exists(index_location):\n",
    "        shutil.rmtree(index_location)\n",
    "        os.makedirs(index_location)\n",
    "    else:\n",
    "        os.makedirs(index_location)\n",
    "    \n",
    "    # generate index input file\n",
    "    trec_answers = answers_to_trec(answers)    \n",
    "    to_trecfile(trec_answers, index_input_file, compression = 'no')\n",
    "    \n",
    "    # build index\n",
    "    build_index(index_input_dir, index_location)\n",
    "    \n",
    "    # generate query file\n",
    "    trec_query = query_to_trec(query_id, query)\n",
    "    to_trecfile(trec_query, trec_query_file, compression = 'no')\n",
    "    \n",
    "    \n",
    "    # generate subtitle query file\n",
    "    trec_sub = query_to_trec(query_id, sub)\n",
    "    to_trecfile(trec_sub, trec_sub_file, compression = 'no')\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_build_index(questions_data):\n",
    "    # Multiprocessing stuff\n",
    "\n",
    "    make_folder(all_index_dir)\n",
    "    make_folder(all_index_inputs)\n",
    "    make_folder(all_query_files)\n",
    "    make_folder(all_sub_files)\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                               )\n",
    "    pool_outputs = pool.map_async(build_all_indexes, questions_data)\n",
    "\n",
    "    pool.close() # no more tasks\n",
    "    while (True):\n",
    "        if (pool_outputs.ready()): break\n",
    "        remaining = pool_outputs._number_left\n",
    "#         remaining2 = remaining1\n",
    "#         remaining1 = pool_outputs._number_left\n",
    "        if remaining%10 == 0:\n",
    "            print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "      \n",
    "    pool.join()  # wrap up current tasks\n",
    "    pool_outputs.get()\n",
    "        \n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_trec(q_id, query):\n",
    "    q_t = {}\n",
    "    q_t[q_id] = '<top>\\n\\n' +         '<num> Number: ' + str(q_id) + '\\n' +         '<title> ' + query + '\\n\\n' +         '<desc> Description:' + '\\n\\n' +         '<narr> Narrative:' + '\\n\\n' +         '</top>\\n\\n'\n",
    "    return q_t\n",
    "    \n",
    "#     queries_list = []\n",
    "#     queries_dict = {}\n",
    "#     query = {}\n",
    "#     id_num = 0\n",
    "#     ids_dict = {}\n",
    "#     q_trec = {}\n",
    "#     for query in q_dup_pos:\n",
    "#         str_id = str(id_num)\n",
    "#         id_new = str_id.rjust(15, '0')\n",
    "        \n",
    "#         key = query['doc_id']\n",
    "#         q = questions[key]\n",
    "# #         print(key)\n",
    "#         text = remove_sc(q['title'] + ' ' + q['text']) #Join title and text \n",
    "#         query['number'] = key\n",
    "# #         query['text'] = '#stopword(' + text + ')'\n",
    "#         query['text'] = '(' + text + ')'\n",
    "#         queries_list.append(dict(query))\n",
    "        \n",
    "#         q_t = '<top>\\n\\n' +           '<num> Number: ' + id_new + '\\n' +           '<title> ' + text + '\\n\\n' +           '<desc> Description:' + '\\n\\n' +           '<narr> Narrative:' + '\\n\\n' +           '</top>\\n\\n'\n",
    "#         q_trec[key] = q_t\n",
    "# #         print(q)\n",
    "#         ids_dict[str(id_num)] = key\n",
    "#         id_num += 1\n",
    "        \n",
    "#     queries_dict['queries'] = queries_list\n",
    "#     # with open(filename, 'wt', encoding='utf-8') as q_file:\n",
    "#     with open(filename, 'wt') as q_file: #encoding option not working on python 2.7\n",
    "#         json.dump(queries_dict, q_file, indent = 4)\n",
    "        \n",
    "#     return [q_trec, ids_dict]\n",
    "        \n",
    "#         ########################\n",
    "#         ########################\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, hits, b=0.2, k=0.8, N=10, M=10, Lambda=0.5):\n",
    "#     print(q_topics_file)\n",
    "    #print(hits)\n",
    "    anserini_search = anserini_loc + 'target/appassembler/bin/SearchCollection'\n",
    "    command = [ \n",
    "               'sh',\n",
    "               anserini_search,\n",
    "               '-topicreader',\n",
    "                'Trec',\n",
    "                '-index',\n",
    "                index_loc,\n",
    "                '-topics',\n",
    "                q_topics_file,\n",
    "                '-output',\n",
    "                retrieved_docs_file,\n",
    "                '-bm25',\n",
    "                '-b',\n",
    "                str(b),\n",
    "                '-k1',\n",
    "                str(k),\n",
    "                '-rm3',\n",
    "                '-rm3.fbDocs',\n",
    "                str(int(N)),\n",
    "                '-rm3.fbTerms',\n",
    "                str(int(M)),\n",
    "                '-rm3.originalQueryWeight',\n",
    "                str(Lambda),\n",
    "                '-hits',\n",
    "                str(hits), \n",
    "                '-threads',\n",
    "                '10'\n",
    "               ]\n",
    "#     print(command)\n",
    "#     command = command.encode('utf-8')\n",
    "    anserini_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = anserini_exec.communicate()\n",
    "#     print(out)\n",
    "#     print('Searching error: ', err)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preds_file(retrieved_docs_file):\n",
    "    \n",
    "    with open(retrieved_docs_file, 'rt') as f_in:\n",
    "        try: \n",
    "            for doc in f_in:\n",
    "#                 print(doc)\n",
    "                q_id = doc.split(' ')[0]\n",
    "#                 print(doc.split(' ')[2])\n",
    "                pred_ans_id = doc.split(' ')[2]\n",
    "\n",
    "            return pred_ans_id\n",
    "        except:\n",
    "            pred_ans_id = int(6) # When BM25+RM3 does not find any document, return an answer index outside the valid answer range\n",
    "            return pred_ans_id\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_compute(q_data, b,k,N,M,Lambda):\n",
    "    query_id = q_data[0]\n",
    "    query = q_data[1] # either question, subtitle, retrieval model?\n",
    "    answers = q_data[2] # answers, always the same set\n",
    "#     print(query_id)\n",
    "    \n",
    "#     temp_dir = workdir + str(query_id) + '_temp'  + '/'\n",
    "#     temp_index_input_dir = temp_dir + 'input_to_index/'\n",
    "#     temp_index_dir = temp_dir + 'index/'\n",
    "#     temp_index_input_file = temp_index_input_dir + 'trec_doc_input_file'\n",
    "#     temp_trec_query_file = temp_dir + 'trec_query_file'\n",
    "    index_location = all_index_dir + str(query_id) + '_index' \n",
    "    trec_query_file = all_query_files + str(query_id) + 'trec_query_file'\n",
    "    retrieved_doc_file = all_retrieved_files + str(query_id) + '_retrieved_doc_file'\n",
    "    \n",
    "    \n",
    "#     if os.path.exists(temp_dir):\n",
    "#         shutil.rmtree(temp_dir)\n",
    "#         os.makedirs(temp_dir)\n",
    "#         os.makedirs(temp_index_input_dir)\n",
    "#     if not os.path.exists(temp_dir):\n",
    "#         os.makedirs(temp_dir)\n",
    "#         os.makedirs(temp_index_input_dir)\n",
    "    \n",
    "#     # generate index input file\n",
    "#     trec_answers = answers_to_trec(answers)\n",
    "#     to_trecfile(trec_answers, temp_index_input_file, compression = 'no')\n",
    "    \n",
    "#     # build index\n",
    "#     build_index(temp_index_input_dir, temp_index_dir)\n",
    "    \n",
    "#     # generate query file\n",
    "#     trec_query = query_to_trec(query_id, query, temp_trec_query_file)\n",
    "#     to_trecfile(trec_query, temp_trec_query_file, compression = 'no')\n",
    "    \n",
    "    # get baseline scores file\n",
    "    # hits = 1, because we are interested in the closest answer, nothing else\n",
    "    retrieve_docs(trec_query_file, retrieved_doc_file, index_location, hits, b, k, N, M, Lambda)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predicted_answer_id = generate_preds_file(retrieved_doc_file)\n",
    "\n",
    "    # \n",
    "####     shutil.rmtree(temp_index_input_dir)\n",
    "#     shutil.rmtree(temp_dir)\n",
    "    \n",
    "    return predicted_answer_id\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predicted_answers, gold_answers):\n",
    "    \n",
    "    preds = np.asarray(predicted_answers)\n",
    "    targets = np.asarray(gold_answers)\n",
    "    acc = sum(preds == targets) / float(len(preds))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(params):\n",
    "    b = params[0]\n",
    "    k = params[1]\n",
    "    N = params[2]\n",
    "    M = params[3]\n",
    "    Lambda = params[4]\n",
    "    \n",
    "    pred_answers = []\n",
    "    gold_answers = []\n",
    "    for item in questions_data:\n",
    "        answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "        if model_type == 'qa':\n",
    "            q_data = [item['qid'], item['q'], answers]\n",
    "        elif model_type == 'sa':\n",
    "            q_data = [item['qid'], item['q'], answers]\n",
    "        elif model_type == 'retrieval':\n",
    "            print('Retrieval model to be built...')\n",
    "#             q_data = [item['qid'], item['q'], answers]\n",
    "\n",
    "        predicted_answer_id = baseline_compute(q_data,b,k,N,M,Lambda)\n",
    "        pred_answers.append(int(predicted_answer_id))\n",
    "        gold_answers.append(int(item['answer_idx']))\n",
    "    \n",
    "    acc = evaluate(pred_answers, gold_answers)\n",
    "    results = [\n",
    "        b,\n",
    "        k,\n",
    "        N,\n",
    "        M,\n",
    "        Lambda,\n",
    "        float(acc)\n",
    "    ]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_params(hyper_params, num_iter):\n",
    "    random_h_params_list = []\n",
    "    while len(random_h_params_list) < num_iter:\n",
    "        random_h_params_set = []\n",
    "        for h_param_list in hyper_params:\n",
    "            sampled_h_param = random.sample(list(h_param_list), k=1)\n",
    "#             print(type(sampled_h_param[0]))\n",
    "#             print(sampled_h_param[0])\n",
    "            random_h_params_set.append(round(sampled_h_param[0], 3))\n",
    "        if not random_h_params_set in random_h_params_list:\n",
    "            random_h_params_list.append(random_h_params_set)\n",
    "#             print('Non repeated')\n",
    "        else:\n",
    "            print('repeated')\n",
    "    return random_h_params_list\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_dev_model(best_model_params_file, n_rand_iter, pool_size):\n",
    "#     random_search = 'yes'\n",
    "    make_folder(all_retrieved_files)\n",
    "    if random_search == 'yes':\n",
    "        ## Heavy random search\n",
    "        brange = np.arange(0.1,1,0.05)\n",
    "        krange = np.arange(0.1,4,0.1)\n",
    "        N_range = np.arange(5,500,1) # num of docs\n",
    "        M_range = np.arange(5,500,1) # num of terms\n",
    "        lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "        ## Light random search\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = np.arange(1,50,2)\n",
    "#         M_range = np.arange(1,50,2)\n",
    "#         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "        h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "        params = get_random_params(h_param_ranges, n_rand_iter)\n",
    "\n",
    "    else:\n",
    "        brange = [0.2]\n",
    "        krange = [0.8]\n",
    "        N_range = [11]\n",
    "        M_range = [10]\n",
    "        lamb_range = [0.5]\n",
    "       \n",
    "        params = [[round(b,3), round(k,3), round(N,3), round(M,3), round(Lambda,3)] \n",
    "                  for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "    \n",
    "#     print(len(params))\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "#     pool_outputs = pool.map(baseline_computing, params)\n",
    "    \n",
    "\n",
    "    pool_outputs = pool.map_async(evaluate_params, params)\n",
    "    print(pool_outputs.get())\n",
    "    ###\n",
    "\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    \n",
    "    pool.close() # no more tasks\n",
    "    while (True):\n",
    "        if (pool_outputs.ready()): break\n",
    "        remaining = pool_outputs._number_left\n",
    "#         remaining2 = remaining1\n",
    "#         remaining1 = pool_outputs._number_left\n",
    "        if remaining%10 == 0:\n",
    "            print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "      \n",
    "    pool.join()  # wrap up current tasks\n",
    "    pool_outputs.get()\n",
    "    params_file = './baselines/best_ir_model/' + 'tvqa' + '_' + 'bm25_rm3_' + data_split + '_hparams.pickle'\n",
    "    pickle.dump(pool_outputs.get(), open(params_file, \"wb\" ) )\n",
    "    print('Total parameters tested: ' + str(len(pool_outputs.get())))\n",
    "    best_model_params = max(pool_outputs.get(), key=lambda x: x[5])\n",
    "    \n",
    "    best_model_dict = {\n",
    "        'b': best_model_params[0],\n",
    "        'k': best_model_params[1],\n",
    "        'N': best_model_params[2],\n",
    "        'M': best_model_params[3],\n",
    "        'Lambda': best_model_params[4],\n",
    "        'n_rand_iter': n_rand_iter,\n",
    "        'hits': hits,\n",
    "        'auc05_score': best_model_params[5],\n",
    "    }\n",
    "    best_model_dict = {k:str(v) for k, v in best_model_dict.items()} # everything to string\n",
    "    \n",
    "    print(best_model_dict)\n",
    "    with open(best_model_params_file, 'wt') as best_model_f:\n",
    "        json.dump(best_model_dict, best_model_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_model(val_data, train_data):\n",
    "    tfidf_vectorizer_q_train = TfidfVectorizer()\n",
    "    gold_answers = []\n",
    "    predicted_answers = []\n",
    "    questions_train = [ele['q'] for ele in train_data]\n",
    "    tfidf_q_train = tfidf_vectorizer_q_train.fit_transform(questions_train)\n",
    "#     print(tfidf_q_train)\n",
    "    j = 0\n",
    "    for item in val_data:\n",
    "        j += 1\n",
    "#         print(j)\n",
    "    #     print(item['q'])\n",
    "#         print('main: ', tfidf_q_train.shape)\n",
    "        try:\n",
    "            q = [item['q']]\n",
    "    #         q_train = [q for q in train_data['q']]\n",
    "    #         answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "        #     print(answers)\n",
    "    #         print(q)\n",
    "            tfidf_q = tfidf_vectorizer_q_train.transform(q)\n",
    "        #     print(tfidf_q)\n",
    "\n",
    "        #     print(tfidf_answers)\n",
    "            cosine_similarities_q_train = linear_kernel(tfidf_q, tfidf_q_train).flatten()\n",
    "            related_docs_indices_q_train = cosine_similarities_q_train.argsort()[:-5:-1]\n",
    "            q_similar_idx = related_docs_indices_q_train[0]\n",
    "    #         print(q_similar)\n",
    "            gold_a_idx = train_data[q_similar_idx]['answer_idx']\n",
    "            gold_train_answer = [train_data[q_similar_idx]['a' + str(gold_a_idx)]]\n",
    "    #         print(gold_train_answer)\n",
    "\n",
    "            tfidf_vectorizer_val = TfidfVectorizer()\n",
    "    #         print(gold_train_answer)\n",
    "            tfidf_q_val = tfidf_vectorizer_val.fit_transform(gold_train_answer)\n",
    "    #         print('second: ', tfidf_q_val.shape)\n",
    "\n",
    "            answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "            tfidf_answers = tfidf_vectorizer_val.transform(answers)\n",
    "            cosine_similarities = linear_kernel(tfidf_q_val, tfidf_answers).flatten()\n",
    "            related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "            gold_answers.append(item['answer_idx'])\n",
    "            predicted_answers.append(related_docs_indices[0])\n",
    "        \n",
    "        except: \n",
    "            print(train_data[q_similar_idx])\n",
    "        \n",
    "        if j%1000 == 0:\n",
    "            print('processed: ', j)\n",
    "#         gold_answers.append(item['answer_idx'])\n",
    "#         predicted_answers.append(related_docs_indices[0])\n",
    "    return [predicted_answers, gold_answers]\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder(folder):\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "        os.makedirs(folder)\n",
    "    else:\n",
    "        os.makedirs(folder)\n",
    "\n",
    "\n",
    "#############\n",
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-09 16:44:30.075647\n",
      "Dev mode: \n",
      "('Total elements: ', 24408)\n",
      "('Starting', 'PoolWorker-3')\n",
      "('Waiting for', 0, 'tasks to complete...')\n",
      "('Starting', 'PoolWorker-4')\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "[[0.9, 2.5, 231.0, 158.0, 0.5, 0.2], [0.15, 3.1, 185.0, 318.0, 0.4, 0.3]]\n",
      "Total parameters tested: 2\n",
      "{'hits': '1', 'b': '0.15', 'k': '3.1', 'M': '318.0', 'N': '185.0', 'auc05_score': '0.3', 'n_rand_iter': '2', 'Lambda': '0.4'}\n",
      "2019-05-09 16:45:05.515996\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "# def main(args):\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    dev_file = './data/tvqa_new_dev_processed.json' # Original train data was split in new_train, new_dev\n",
    "    test_file = './data/tvqa_val_processed.json' # Val is being used as test!\n",
    "    build_index_flag = 'yes'\n",
    "    random_search = 'yes'\n",
    "    workdir = './workdir/'\n",
    "    \n",
    "    hits = 1\n",
    "    \n",
    "    anserini_loc = '../anserini/'\n",
    "    n_rand_iter = 2\n",
    "    pool_size = 1\n",
    "    \n",
    "    with open(dev_file, 'r') as f:\n",
    "        processed_data_dev = json.load(f)\n",
    "\n",
    "    with open(test_file, 'r') as f:\n",
    "        processed_data_test = json.load(f)\n",
    "    \n",
    "    # Options\n",
    "    data_split = 'dev'\n",
    "    model_type = 'qa'\n",
    "#     model_type = 'sa'\n",
    "#     model_type = 'retrieval'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(workdir):\n",
    "        os.makedirs(workdir)\n",
    "    \n",
    "    if data_split == 'dev':\n",
    "        print('Dev mode: ')\n",
    "        print('Total elements: ', len(processed_data_dev))\n",
    "        questions_data = processed_data_dev\n",
    "\t\n",
    "    \tall_index_dir = workdir + 'index_dirs_dev/'\n",
    "    \tall_index_inputs = workdir + 'index_inputs_dev/'\n",
    "        all_query_files = workdir + 'query_files_dev/'\n",
    "        all_sub_files = workdir + 'sub_files_dev/'\n",
    "        all_retrieved_files = workdir + 'retrieved_files_dev/'\n",
    "        \n",
    "    elif data_split == 'test':\n",
    "        print('Test mode: ')\n",
    "        questions_data = processed_data_test\n",
    "\t\n",
    "    \tall_index_dir = workdir + 'index_dirs_test/'\n",
    "    \tall_index_inputs = workdir + 'index_inputs_test/'\n",
    "        all_query_files = workdir + 'query_files_test/'\n",
    "        all_sub_files = workdir + 'sub_files_test/'\n",
    "        all_retrieved_files = workdir + 'retrieved_files_dev/'\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    questions_data = questions_data[0:10]\n",
    "    call_build_index(questions_data)\n",
    "\n",
    "    best_model_params_file = './baselines/best_ir_model/tvqa_bm25_rm3_best_model_dev.json'\n",
    "    \n",
    "    find_best_dev_model(best_model_params_file, n_rand_iter, pool_size)\n",
    "    end = datetime.datetime.now()\n",
    "    print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     argparser = argparse.ArgumentParser(sys.argv[0], conflict_handler='resolve')\n",
    "#     argparser.add_argument(\"--cuda\", action=\"store_true\")\n",
    "#     argparser.add_argument(\"--run_dir\",  type=str, default=\"/D/home/tao/mnt/ASAPPNAS/tao/test\")\n",
    "#     argparser.add_argument(\"--model\", type=str, required=True, help=\"which model class to use\")\n",
    "#     argparser.add_argument(\"--embedding\", \"--emb\", type=str, help=\"path of embedding\")\n",
    "#     argparser.add_argument(\"--train\", type=str, required=True, help=\"training file\")\n",
    "#     argparser.add_argument(\"--wasserstein\", action=\"store_true\")\n",
    "#     argparser.add_argument(\"--cross_train\", type=str, required=True, help=\"cross training file\")\n",
    "#     argparser.add_argument(\"--eval\", type=str, required=True, help=\"validation file\")\n",
    "#     argparser.add_argument(\"--batch_size\", \"--batch\", type=int, default=100)\n",
    "#     argparser.add_argument(\"--max_epoch\", type=int, default=100)\n",
    "#     argparser.add_argument(\"--learning\", type=str, default='adam')\n",
    "#     argparser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "#     argparser.add_argument(\"--lr2\", type=float, default=0.0001)\n",
    "#     argparser.add_argument(\"--lambda_d\", type=float, default=0.01)\n",
    "#     argparser.add_argument(\"--use_content\", action=\"store_true\")\n",
    "#     argparser.add_argument(\"--eval_use_content\", action=\"store_true\")\n",
    "#     argparser.add_argument(\"--save_model\", type=str, required=False, help=\"location to save model\")\n",
    "\n",
    "#     args, _  = argparser.parse_known_args()\n",
    "#     main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
