{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re \n",
    "import csv\n",
    "import torch\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import uuid\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import bz2\n",
    "import pandas as pd\n",
    "# import dbmanager  as dbmanager\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "# nltk.download('punkt') # for english sentences tokenization\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load non processed data\n",
    "# file = './data/tvqa_qa_release/tvqa_train.jsonl'\n",
    "# with open(file, 'r') as f:\n",
    "#     lines = []\n",
    "#     for l in f.readlines():\n",
    "#         loaded_l = json.loads(l.strip(\"\\n\"))\n",
    "#         lines.append(loaded_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_to_trec(answers):\n",
    "    trec_answers = {}\n",
    "    key = 0\n",
    "    for answer in answers:\n",
    "        print(key, '_', answer)\n",
    "#         print(answer)\n",
    "        doc = '<DOC>\\n' + \\\n",
    "            '<DOCNO>' + str(key) + '</DOCNO>\\n' + \\\n",
    "            '<TITLE>' + answer + '</TITLE>\\n' + \\\n",
    "            '</DOC>\\n'\n",
    "        trec_answers[str(key)] = doc\n",
    "        key += 1\n",
    "    return trec_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_index_dir():\n",
    "\n",
    "# try:\n",
    "#                 golden_file = sys.argv[1]\n",
    "#                 predictions_file = sys.argv[2]\n",
    "#         except:\n",
    "#                 sys.exit(\"Provide golden and predictions files.\")\n",
    "        \n",
    "#         try:\n",
    "#                 system_name = sys.argv[3]\n",
    "#         except :\n",
    "#                 try:\n",
    "#                         system_name = predictions_file.split('/')[-1]\n",
    "#                 except:\n",
    "#                         system_name = predictions_file\n",
    "\n",
    "#         with open(golden_file, 'r') as f:\n",
    "#                 golden_data = json.load(f)\n",
    "\n",
    "#         with open(predictions_file, 'r') as f:\n",
    "#                 predictions_data = json.load(f)\n",
    "\n",
    "#         temp_dir = uuid.uuid4().hex\n",
    "#         qrels_temp_file = '{0}/{1}'.format(temp_dir, 'qrels.txt')\n",
    "#         qret_temp_file = '{0}/{1}'.format(temp_dir, 'qret.txt')\n",
    "\n",
    "#         try:\n",
    "#                 if not os.path.exists(temp_dir):\n",
    "#                         os.makedirs(temp_dir)\n",
    "#                 else:\n",
    "#                         sys.exit(\"Possible uuid collision\")\n",
    "\n",
    "#                 format_bioasq2treceval_qrels(golden_data, qrels_temp_file)\n",
    "#                 format_bioasq2treceval_qret(predictions_data, system_name, qret_temp_file)\n",
    "\n",
    "#                 trec_evaluate(qrels_temp_file, qret_temp_file)\n",
    "#         finally:\n",
    "#                 os.remove(qrels_temp_file)\n",
    "#                 os.remove(qret_temp_file)\n",
    "#                 os.rmdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trecfile(docs, filename, compression = 'yes'):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    if compression == 'yes':\n",
    "        with gzip.open(filename,'wt') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)\n",
    "    else:\n",
    "        with open(filename,'wt') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(index_input, index_loc):\n",
    "    if build_index_flag == 'no':\n",
    "        return\n",
    "# Build corpus index \n",
    "    if os.path.exists(index_loc):\n",
    "        shutil.rmtree(index_loc)\n",
    "        os.makedirs(index_loc)\n",
    "    else:\n",
    "        os.makedirs(index_loc) \n",
    "#     index_loc_param = '--indexPath=' + index_loc\n",
    "\n",
    "    anserini_index = anserini_loc + 'target/appassembler/bin/IndexCollection'\n",
    "    anserini_parameters = [\n",
    "#                            'nohup', \n",
    "                           'sh',\n",
    "                           anserini_index,\n",
    "                           '-collection',\n",
    "                           'TrecCollection',\n",
    "                           '-generator',\n",
    "                           'JsoupGenerator',\n",
    "                           '-threads',\n",
    "                            '16',\n",
    "                            '-input',\n",
    "                           index_input,\n",
    "                           '-index',\n",
    "                           index_loc,\n",
    "                           '-storePositions',\n",
    "                            '-keepStopwords',\n",
    "                            '-storeDocvectors',\n",
    "                            '-storeRawDocs']\n",
    "#                           ' >& ',\n",
    "#                           log_file,\n",
    "#                            '&']\n",
    "\n",
    "\n",
    "\n",
    "#     anserini_parameters = ['ls',\n",
    "#                           index_loc]\n",
    "\n",
    "\n",
    "    print(anserini_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(anserini_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print('Index error: ', err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_trec(q_id, query, filename):\n",
    "    q_t = {}\n",
    "    q_t[q_id] = '<top>\\n\\n' + \\\n",
    "        '<num> Number: ' + str(q_id) + '\\n' + \\\n",
    "        '<title> ' + query + '\\n\\n' + \\\n",
    "        '<desc> Description:' + '\\n\\n' + \\\n",
    "        '<narr> Narrative:' + '\\n\\n' + \\\n",
    "        '</top>\\n\\n'\n",
    "    return q_t\n",
    "    \n",
    "#     queries_list = []\n",
    "#     queries_dict = {}\n",
    "#     query = {}\n",
    "#     id_num = 0\n",
    "#     ids_dict = {}\n",
    "#     q_trec = {}\n",
    "#     for query in q_dup_pos:\n",
    "#         str_id = str(id_num)\n",
    "#         id_new = str_id.rjust(15, '0')\n",
    "        \n",
    "#         key = query['doc_id']\n",
    "#         q = questions[key]\n",
    "# #         print(key)\n",
    "#         text = remove_sc(q['title'] + ' ' + q['text']) #Join title and text \n",
    "#         query['number'] = key\n",
    "# #         query['text'] = '#stopword(' + text + ')'\n",
    "#         query['text'] = '(' + text + ')'\n",
    "#         queries_list.append(dict(query))\n",
    "        \n",
    "#         q_t = '<top>\\n\\n' +           '<num> Number: ' + id_new + '\\n' +           '<title> ' + text + '\\n\\n' +           '<desc> Description:' + '\\n\\n' +           '<narr> Narrative:' + '\\n\\n' +           '</top>\\n\\n'\n",
    "#         q_trec[key] = q_t\n",
    "# #         print(q)\n",
    "#         ids_dict[str(id_num)] = key\n",
    "#         id_num += 1\n",
    "        \n",
    "#     queries_dict['queries'] = queries_list\n",
    "#     # with open(filename, 'wt', encoding='utf-8') as q_file:\n",
    "#     with open(filename, 'wt') as q_file: #encoding option not working on python 2.7\n",
    "#         json.dump(queries_dict, q_file, indent = 4)\n",
    "        \n",
    "#     return [q_trec, ids_dict]\n",
    "        \n",
    "#         ########################\n",
    "#         ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, hits, b=0.2, k=0.8, N=10, M=10, Lambda=0.5):\n",
    "    print(q_topics_file)\n",
    "    #print(hits)\n",
    "    anserini_search = anserini_loc + 'target/appassembler/bin/SearchCollection'\n",
    "    command = [ \n",
    "               'sh',\n",
    "               anserini_search,\n",
    "               '-topicreader',\n",
    "                'Trec',\n",
    "                '-index',\n",
    "                index_loc,\n",
    "                '-topics',\n",
    "                q_topics_file,\n",
    "                '-output',\n",
    "                retrieved_docs_file,\n",
    "                '-bm25',\n",
    "                '-b',\n",
    "                str(b),\n",
    "                '-k1',\n",
    "                str(k),\n",
    "                '-rm3',\n",
    "                '-rm3.fbDocs',\n",
    "                str(N),\n",
    "                '-rm3.fbTerms',\n",
    "                str(M),\n",
    "                '-rm3.originalQueryWeight',\n",
    "                str(Lambda),\n",
    "                '-hits',\n",
    "                str(hits), \n",
    "                '-threads',\n",
    "                '10'\n",
    "               ]\n",
    "    print(command)\n",
    "#     command = command.encode('utf-8')\n",
    "    anserini_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = anserini_exec.communicate()\n",
    "#     print(out)\n",
    "    print('Searching error: ', err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preds_file(retrieved_docs_file):\n",
    "    \n",
    "    with open(retrieved_docs_file, 'rt') as f_in:\n",
    "        try: \n",
    "            for doc in f_in:\n",
    "                print(doc)\n",
    "                q_id = doc.split(' ')[0]\n",
    "                print(doc.split(' ')[2])\n",
    "                pred_ans_id = doc.split(' ')[2]\n",
    "\n",
    "            return pred_ans_id\n",
    "        except:\n",
    "            pred_ans_id = int(6) # When BM25+RM3 does not find any document, return an answer index outside the valid answer range\n",
    "            return pred_ans_id\n",
    "    \n",
    "    \n",
    "#             current_key = ids_dict[id_aux]\n",
    "#             key_pair = current_key + '_' + doc.split(' ')[2]\n",
    "#             all_dict[key_pair] = doc.split(' ')[4]\n",
    "#         bm25_scores = [] \n",
    "#         i = 0\n",
    "#         for query_dict in q_all:\n",
    "#             i += 1\n",
    "#             key_pair = query_dict['doc_id'] + '_' + query_dict['dup_id']\n",
    "#             try: \n",
    "#                 query_dict['score'] = all_dict[key_pair]\n",
    "#             except:\n",
    "#                 query_dict['score'] = 0\n",
    "# #            if i % 10000 == 0:\n",
    "# #                 print('processed: ', i)\n",
    "#             bm25_scores.append(dict(query_dict))\n",
    "#         return bm25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_compute(q_data, b,k,N,M,Lambda):\n",
    "    query_id = q_data[0]\n",
    "    query = q_data[1] # either question, subtitle, retrieval model?\n",
    "    answers = q_data[2] # answers, always the same set\n",
    "    print(query_id)\n",
    "    \n",
    "    temp_dir = workdir + str(query_id) + '_temp'  + '/'\n",
    "    temp_index_input_dir = temp_dir + 'input_to_index/'\n",
    "    temp_index_dir = temp_dir + 'index/'\n",
    "    temp_index_input_file = temp_index_input_dir + 'trec_doc_input_file'\n",
    "    temp_trec_query_file = temp_dir + 'trec_query_file'\n",
    "    temp_retrieved_doc_file = temp_dir + 'retrieved_doc_file'\n",
    "    \n",
    "    \n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        os.makedirs(temp_dir)\n",
    "        os.makedirs(temp_index_input_dir)\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "        os.makedirs(temp_index_input_dir)\n",
    "    \n",
    "    # generate index input file\n",
    "    trec_answers = answers_to_trec(answers)\n",
    "    to_trecfile(trec_answers, temp_index_input_file, compression = 'no')\n",
    "    \n",
    "    # build index\n",
    "    build_index(temp_index_input_dir, temp_index_dir)\n",
    "    \n",
    "    # generate query file\n",
    "    trec_query = query_to_trec(query_id, query, temp_trec_query_file)\n",
    "    to_trecfile(trec_query, temp_trec_query_file, compression = 'no')\n",
    "    \n",
    "    # get baseline scores file\n",
    "    # hits = 1, because we are interested in the closest answer, nothing else\n",
    "    retrieve_docs(temp_trec_query_file, temp_retrieved_doc_file, temp_index_dir, hits, b, k, N, M, Lambda)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predicted_answer_id = generate_preds_file(temp_retrieved_doc_file)\n",
    "\n",
    "    # \n",
    "####     shutil.rmtree(temp_index_input_dir)\n",
    "#     shutil.rmtree(temp_dir)\n",
    "    \n",
    "    return predicted_answer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predicted_answers, gold_answers):\n",
    "    preds = np.asarray(predicted_answers)\n",
    "    targets = np.asarray(gold_answers)\n",
    "    acc = sum(preds == targets) / float(len(preds))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(params):\n",
    "    b = params[0]\n",
    "    k = params[1]\n",
    "    N = params[2]\n",
    "    M = params[3]\n",
    "    Lambda = params[4]\n",
    "    \n",
    "    pred_answers = []\n",
    "    gold_answers = []\n",
    "    for item in questions_data:\n",
    "        answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "        if model_type == 'qa':\n",
    "            q_data = [item['qid'], item['q'], answers]\n",
    "        elif model_type == 'sa':\n",
    "            q_data = [item['qid'], item['q'], answers]\n",
    "        elif model_type == 'retrieval':\n",
    "            print('Retrieval model to be built...')\n",
    "#             q_data = [item['qid'], item['q'], answers]\n",
    "\n",
    "        predicted_answer_id = baseline_compute(q_data,b,k,N,M,Lambda)\n",
    "        pred_answers.append(predicted_answer_id)\n",
    "        gold_answers.append(item['answer_idx'])\n",
    "    \n",
    "    acc = evaluate(predicted_answers, gold_answers)\n",
    "    results = [\n",
    "        b,\n",
    "        k,\n",
    "        N,\n",
    "        M,\n",
    "        Lambda,\n",
    "        float(acc)\n",
    "    ]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits = 1\n",
    "# params = [1,1,5,1,1]\n",
    "# evaluate_params(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_params(hyper_params, num_iter):\n",
    "    random_h_params_list = []\n",
    "    while len(random_h_params_list) < num_iter:\n",
    "        random_h_params_set = []\n",
    "        for h_param_list in hyper_params:\n",
    "            sampled_h_param = random.sample(list(h_param_list), k=1)\n",
    "#             print(type(sampled_h_param[0]))\n",
    "#             print(sampled_h_param[0])\n",
    "            random_h_params_set.append(round(sampled_h_param[0], 3))\n",
    "        if not random_h_params_set in random_h_params_list:\n",
    "            random_h_params_list.append(random_h_params_set)\n",
    "#             print('Non repeated')\n",
    "        else:\n",
    "            print('repeated')\n",
    "    return random_h_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_dev_model(best_model_params_file, n_rand_iter, pool_size):\n",
    "#     random_search = 'yes'\n",
    "    \n",
    "    if random_search == 'yes':\n",
    "        ## Heavy random search\n",
    "        brange = np.arange(0.1,1,0.05)\n",
    "        krange = np.arange(0.1,4,0.1)\n",
    "        N_range = np.arange(5,500,1) # num of docs\n",
    "        M_range = np.arange(5,500,1) # num of terms\n",
    "        lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "        ## Light random search\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = np.arange(1,50,2)\n",
    "#         M_range = np.arange(1,50,2)\n",
    "#         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "        h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "        params = get_random_params(h_param_ranges, n_rand_iter)\n",
    "\n",
    "    else:\n",
    "        brange = [0.2]\n",
    "        krange = [0.8]\n",
    "        N_range = [11]\n",
    "        M_range = [10]\n",
    "        lamb_range = [0.5]\n",
    "       \n",
    "        params = [[round(b,3), round(k,3), round(N,3), round(M,3), round(Lambda,3)] \n",
    "                  for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "    \n",
    "#     print(len(params))\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "#     pool_outputs = pool.map(baseline_computing, params)\n",
    "    \n",
    "\n",
    "    pool_outputs = pool.map_async(evaluate_params, params)\n",
    "    print(pool_outputs.get())\n",
    "    ###\n",
    "\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    \n",
    "    pool.close() # no more tasks\n",
    "    while (True):\n",
    "        if (pool_outputs.ready()): break\n",
    "        remaining = pool_outputs._number_left\n",
    "#         remaining2 = remaining1\n",
    "#         remaining1 = pool_outputs._number_left\n",
    "        if remaining%10 == 0:\n",
    "            print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "      \n",
    "    pool.join()  # wrap up current tasks\n",
    "    pool_outputs.get()\n",
    "    params_file = './baselines/best_ir_model/' + 'tvqa' + '_' + 'bm25_rm3_' + data_split + '_hparams.pickle'\n",
    "    pickle.dump(pool_outputs.get(), open(params_file, \"wb\" ) )\n",
    "    print('Total parameters tested: ' + str(len(pool_outputs.get())))\n",
    "    best_model_params = max(pool_outputs.get(), key=lambda x: x[5])\n",
    "    \n",
    "    best_model_dict = {\n",
    "        'b': best_model_params[0],\n",
    "        'k': best_model_params[1],\n",
    "        'N': best_model_params[2],\n",
    "        'M': best_model_params[3],\n",
    "        'Lambda': best_model_params[4],\n",
    "        'n_rand_iter': n_rand_iter,\n",
    "        'hits': hits,\n",
    "        'auc05_score': best_model_params[5],\n",
    "    }\n",
    "    best_model_dict = {k:str(v) for k, v in best_model_dict.items()} # everything to string\n",
    "    \n",
    "    print(best_model_dict)\n",
    "    with open(best_model_params_file, 'wt') as best_model_f:\n",
    "        json.dump(best_model_dict, best_model_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def qa_prediction(data):\n",
    "#     gold_answers = []\n",
    "#     predicted_answers = []\n",
    "#     print(len(data))\n",
    "#     for item in data:\n",
    "#         tfidf_vectorizer = TfidfVectorizer()\n",
    "#         q = [item['q']]\n",
    "#         answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "#     #     print(answers)\n",
    "#         tfidf_q = tfidf_vectorizer.fit_transform(q)\n",
    "#     #     print(tfidf_q)\n",
    "#         tfidf_answers = tfidf_vectorizer.transform(answers)\n",
    "#     #     print(tfidf_answers)\n",
    "#         cosine_similarities = linear_kernel(tfidf_q, tfidf_answers).flatten()\n",
    "#         related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "#         gold_answers.append(item['answer_idx'])\n",
    "#         predicted_answers.append(related_docs_indices[0])\n",
    "#     return [predicted_answers, gold_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sa_prediction(data):\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "#     gold_answers = []\n",
    "#     predicted_answers = []\n",
    "#     for item in data:\n",
    "#     #     print(item['q'])\n",
    "#         sub = [item['located_sub_text']]\n",
    "#         answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "#     #     print(answers)\n",
    "#         tfidf_sub = tfidf_vectorizer.fit_transform(sub)\n",
    "#     #     print(tfidf_q)\n",
    "#         tfidf_answers = tfidf_vectorizer.transform(answers)\n",
    "#     #     print(tfidf_answers)\n",
    "#         cosine_similarities = linear_kernel(tfidf_sub, tfidf_answers).flatten()\n",
    "#         related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "#         gold_answers.append(item['answer_idx'])\n",
    "#         predicted_answers.append(related_docs_indices[0])\n",
    "#     return [predicted_answers, gold_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [predicted_answers, gold_answers] = sa_prediction(processed_data_val)\n",
    "# preds = np.asarray(predicted_answers)\n",
    "# targets = np.asarray(gold_answers)\n",
    "# acc = sum(preds == targets) / float(len(preds))\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_model(val_data, train_data):\n",
    "    tfidf_vectorizer_q_train = TfidfVectorizer()\n",
    "    gold_answers = []\n",
    "    predicted_answers = []\n",
    "    questions_train = [ele['q'] for ele in train_data]\n",
    "    tfidf_q_train = tfidf_vectorizer_q_train.fit_transform(questions_train)\n",
    "#     print(tfidf_q_train)\n",
    "    j = 0\n",
    "    for item in val_data:\n",
    "        j += 1\n",
    "#         print(j)\n",
    "    #     print(item['q'])\n",
    "#         print('main: ', tfidf_q_train.shape)\n",
    "        try:\n",
    "            q = [item['q']]\n",
    "    #         q_train = [q for q in train_data['q']]\n",
    "    #         answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "        #     print(answers)\n",
    "    #         print(q)\n",
    "            tfidf_q = tfidf_vectorizer_q_train.transform(q)\n",
    "        #     print(tfidf_q)\n",
    "\n",
    "        #     print(tfidf_answers)\n",
    "            cosine_similarities_q_train = linear_kernel(tfidf_q, tfidf_q_train).flatten()\n",
    "            related_docs_indices_q_train = cosine_similarities_q_train.argsort()[:-5:-1]\n",
    "            q_similar_idx = related_docs_indices_q_train[0]\n",
    "    #         print(q_similar)\n",
    "            gold_a_idx = train_data[q_similar_idx]['answer_idx']\n",
    "            gold_train_answer = [train_data[q_similar_idx]['a' + str(gold_a_idx)]]\n",
    "    #         print(gold_train_answer)\n",
    "\n",
    "            tfidf_vectorizer_val = TfidfVectorizer()\n",
    "    #         print(gold_train_answer)\n",
    "            tfidf_q_val = tfidf_vectorizer_val.fit_transform(gold_train_answer)\n",
    "    #         print('second: ', tfidf_q_val.shape)\n",
    "\n",
    "            answers = [item['a' + str(i)] for i in range(0,5)]\n",
    "            tfidf_answers = tfidf_vectorizer_val.transform(answers)\n",
    "            cosine_similarities = linear_kernel(tfidf_q_val, tfidf_answers).flatten()\n",
    "            related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "            gold_answers.append(item['answer_idx'])\n",
    "            predicted_answers.append(related_docs_indices[0])\n",
    "        \n",
    "        except: \n",
    "            print(train_data[q_similar_idx])\n",
    "        \n",
    "        if j%1000 == 0:\n",
    "            print('processed: ', j)\n",
    "#         gold_answers.append(item['answer_idx'])\n",
    "#         predicted_answers.append(related_docs_indices[0])\n",
    "    return [predicted_answers, gold_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mode: \n",
      "('Starting', 'PoolWorker-1')\n",
      "122039\n",
      "(0, '_', u'Because Sheldon is being rude .')\n",
      "(1, '_', u\"Because he does n't like Sheldon .\")\n",
      "(2, '_', u'Because they are having an argument .')\n",
      "(3, '_', u'Because Howard wanted to have a private meal with Raj .')\n",
      "(4, '_', u\"Because Sheldon wo n't loan him money for food .\")\n",
      "['sh', '../anserini/target/appassembler/bin/IndexCollection', '-collection', 'TrecCollection', '-generator', 'JsoupGenerator', '-threads', '16', '-input', './workdir/122039_temp/input_to_index/', '-index', './workdir/122039_temp/index/', '-storePositions', '-keepStopwords', '-storeDocvectors', '-storeRawDocs']\n",
      "2019-05-08 12:44:54,883 INFO  [main] index.IndexCollection (IndexCollection.java:429) - DocumentCollection path: ./workdir/122039_temp/input_to_index/\n",
      "2019-05-08 12:44:54,883 INFO  [main] index.IndexCollection (IndexCollection.java:430) - Index path: ./workdir/122039_temp/index/\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:431) - CollectionClass: TrecCollection\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:432) - Generator: JsoupGenerator\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:433) - Threads: 16\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:434) - Stemmer: porter\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:435) - Keep stopwords? true\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:436) - Store positions? true\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:437) - Store docvectors? true\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:438) - Store transformed docs? false\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:439) - Store raw docs? true\n",
      "2019-05-08 12:44:54,884 INFO  [main] index.IndexCollection (IndexCollection.java:440) - Optimize (merge segments)? false\n",
      "2019-05-08 12:44:54,885 INFO  [main] index.IndexCollection (IndexCollection.java:441) - Whitelist: null\n",
      "2019-05-08 12:44:54,885 INFO  [main] index.IndexCollection (IndexCollection.java:442) - Solr? false\n",
      "2019-05-08 12:44:54,885 INFO  [main] index.IndexCollection (IndexCollection.java:453) - Dry run (no index created)? false\n",
      "2019-05-08 12:44:54,900 INFO  [main] index.IndexCollection (IndexCollection.java:536) - Starting indexer...\n",
      "2019-05-08 12:44:55,127 INFO  [main] index.IndexCollection (IndexCollection.java:563) - 1 files found in ./workdir/122039_temp/input_to_index\n",
      "2019-05-08 12:44:55,321 INFO  [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:295) - input_to_index/trec_doc_input_file: 5 docs added.\n",
      "2019-05-08 12:44:55,564 INFO  [main] index.IndexCollection (IndexCollection.java:638) - # Final Counter Values\n",
      "2019-05-08 12:44:55,564 INFO  [main] index.IndexCollection (IndexCollection.java:639) - indexed:                5\n",
      "2019-05-08 12:44:55,565 INFO  [main] index.IndexCollection (IndexCollection.java:640) - empty:                  0\n",
      "2019-05-08 12:44:55,565 INFO  [main] index.IndexCollection (IndexCollection.java:641) - unindexed:              0\n",
      "2019-05-08 12:44:55,565 INFO  [main] index.IndexCollection (IndexCollection.java:642) - unindexable:            0\n",
      "2019-05-08 12:44:55,565 INFO  [main] index.IndexCollection (IndexCollection.java:643) - skipped:                0\n",
      "2019-05-08 12:44:55,565 INFO  [main] index.IndexCollection (IndexCollection.java:644) - errors:                 0\n",
      "2019-05-08 12:44:55,574 INFO  [main] index.IndexCollection (IndexCollection.java:647) - Total 5 documents indexed in 00:00:00\n",
      "\n",
      "('Index error: ', None)\n",
      "./workdir/122039_temp/trec_query_file\n",
      "['sh', '../anserini/target/appassembler/bin/SearchCollection', '-topicreader', 'Trec', '-index', './workdir/122039_temp/index/', '-topics', './workdir/122039_temp/trec_query_file', '-output', './workdir/122039_temp/retrieved_doc_file', '-bm25', '-b', '0.75', '-k1', '1.2', '-rm3', '-rm3.fbDocs', '281.0', '-rm3.fbTerms', '460.0', '-rm3.originalQueryWeight', '0.3', '-hits', '1', '-threads', '10']\n",
      "('Searching error: ', None)\n",
      "122039\n",
      "(3, '_', u'Because Howard wanted to have a private meal with Raj .')\n",
      "(0, '_', u'Because Sheldon is being rude .')\n",
      "(1, '_', u\"Because he does n't like Sheldon .\")\n",
      "(2, '_', u'Because they are having an argument .')\n",
      "(4, '_', u\"Because Sheldon wo n't loan him money for food .\")\n",
      "['sh', '../anserini/target/appassembler/bin/IndexCollection', '-collection', 'TrecCollection', '-generator', 'JsoupGenerator', '-threads', '16', '-input', './workdir/122039_temp/input_to_index/', '-index', './workdir/122039_temp/index/', '-storePositions', '-keepStopwords', '-storeDocvectors', '-storeRawDocs']\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './workdir/122039_temp/retrieved_doc_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e8434a6f2cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mbest_model_params_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./baselines/best_ir_model/tvqa_bm25_rm3_best_model_dev.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mfind_best_dev_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_params_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rand_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-478c1703cd4d>\u001b[0m in \u001b[0;36mfind_best_dev_model\u001b[0;34m(best_model_params_file, n_rand_iter, pool_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpool_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/francisco/miniconda3/envs/tvqa-gpu-py27/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './workdir/122039_temp/retrieved_doc_file'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-08 12:44:57,540 INFO  [main] index.IndexCollection (IndexCollection.java:429) - DocumentCollection path: ./workdir/122039_temp/input_to_index/\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:430) - Index path: ./workdir/122039_temp/index/\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:431) - CollectionClass: TrecCollection\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:432) - Generator: JsoupGenerator\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:433) - Threads: 16\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:434) - Stemmer: porter\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:435) - Keep stopwords? true\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:436) - Store positions? true\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:437) - Store docvectors? true\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:438) - Store transformed docs? false\n",
      "2019-05-08 12:44:57,541 INFO  [main] index.IndexCollection (IndexCollection.java:439) - Store raw docs? true\n",
      "2019-05-08 12:44:57,542 INFO  [main] index.IndexCollection (IndexCollection.java:440) - Optimize (merge segments)? false\n",
      "2019-05-08 12:44:57,542 INFO  [main] index.IndexCollection (IndexCollection.java:441) - Whitelist: null\n",
      "2019-05-08 12:44:57,542 INFO  [main] index.IndexCollection (IndexCollection.java:442) - Solr? false\n",
      "2019-05-08 12:44:57,542 INFO  [main] index.IndexCollection (IndexCollection.java:453) - Dry run (no index created)? false\n",
      "2019-05-08 12:44:57,556 INFO  [main] index.IndexCollection (IndexCollection.java:536) - Starting indexer...\n",
      "2019-05-08 12:44:57,749 INFO  [main] index.IndexCollection (IndexCollection.java:563) - 1 files found in ./workdir/122039_temp/input_to_index\n",
      "2019-05-08 12:44:57,908 INFO  [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:295) - input_to_index/trec_doc_input_file: 5 docs added.\n",
      "2019-05-08 12:44:58,153 INFO  [main] index.IndexCollection (IndexCollection.java:638) - # Final Counter Values\n",
      "2019-05-08 12:44:58,154 INFO  [main] index.IndexCollection (IndexCollection.java:639) - indexed:                5\n",
      "2019-05-08 12:44:58,154 INFO  [main] index.IndexCollection (IndexCollection.java:640) - empty:                  0\n",
      "2019-05-08 12:44:58,154 INFO  [main] index.IndexCollection (IndexCollection.java:641) - unindexed:              0\n",
      "2019-05-08 12:44:58,154 INFO  [main] index.IndexCollection (IndexCollection.java:642) - unindexable:            0\n",
      "2019-05-08 12:44:58,154 INFO  [main] index.IndexCollection (IndexCollection.java:643) - skipped:                0\n",
      "2019-05-08 12:44:58,155 INFO  [main] index.IndexCollection (IndexCollection.java:644) - errors:                 0\n",
      "2019-05-08 12:44:58,162 INFO  [main] index.IndexCollection (IndexCollection.java:647) - Total 5 documents indexed in 00:00:00\n",
      "\n",
      "('Index error: ', None)\n",
      "./workdir/122039_temp/trec_query_file\n",
      "['sh', '../anserini/target/appassembler/bin/SearchCollection', '-topicreader', 'Trec', '-index', './workdir/122039_temp/index/', '-topics', './workdir/122039_temp/trec_query_file', '-output', './workdir/122039_temp/retrieved_doc_file', '-bm25', '-b', '0.45', '-k1', '1.3', '-rm3', '-rm3.fbDocs', '384.0', '-rm3.fbTerms', '298.0', '-rm3.originalQueryWeight', '0.6', '-hits', '1', '-threads', '10']\n",
      "('Searching error: ', None)\n",
      "122039\n",
      "(0, '_', u'Because Sheldon is being rude .')\n",
      "(1, '_', u\"Because he does n't like Sheldon .\")\n",
      "(2, '_', u'Because they are having an argument .')\n",
      "(3, '_', u'Because Howard wanted to have a private meal with Raj .')\n",
      "(4, '_', u\"Because Sheldon wo n't loan him money for food .\")\n",
      "['sh', '../anserini/target/appassembler/bin/IndexCollection', '-collection', 'TrecCollection', '-generator', 'JsoupGenerator', '-threads', '16', '-input', './workdir/122039_temp/input_to_index/', '-index', './workdir/122039_temp/index/', '-storePositions', '-keepStopwords', '-storeDocvectors', '-storeRawDocs']\n",
      "2019-05-08 12:45:00,256 INFO  [main] index.IndexCollection (IndexCollection.java:429) - DocumentCollection path: ./workdir/122039_temp/input_to_index/\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:430) - Index path: ./workdir/122039_temp/index/\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:431) - CollectionClass: TrecCollection\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:432) - Generator: JsoupGenerator\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:433) - Threads: 16\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:434) - Stemmer: porter\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:435) - Keep stopwords? true\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:436) - Store positions? true\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:437) - Store docvectors? true\n",
      "2019-05-08 12:45:00,257 INFO  [main] index.IndexCollection (IndexCollection.java:438) - Store transformed docs? false\n",
      "2019-05-08 12:45:00,258 INFO  [main] index.IndexCollection (IndexCollection.java:439) - Store raw docs? true\n",
      "2019-05-08 12:45:00,258 INFO  [main] index.IndexCollection (IndexCollection.java:440) - Optimize (merge segments)? false\n",
      "2019-05-08 12:45:00,258 INFO  [main] index.IndexCollection (IndexCollection.java:441) - Whitelist: null\n",
      "2019-05-08 12:45:00,258 INFO  [main] index.IndexCollection (IndexCollection.java:442) - Solr? false\n",
      "2019-05-08 12:45:00,258 INFO  [main] index.IndexCollection (IndexCollection.java:453) - Dry run (no index created)? false\n",
      "2019-05-08 12:45:00,272 INFO  [main] index.IndexCollection (IndexCollection.java:536) - Starting indexer...\n",
      "2019-05-08 12:45:00,472 INFO  [main] index.IndexCollection (IndexCollection.java:563) - 1 files found in ./workdir/122039_temp/input_to_index\n",
      "2019-05-08 12:45:00,653 INFO  [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:295) - input_to_index/trec_doc_input_file: 5 docs added.\n",
      "2019-05-08 12:45:00,929 INFO  [main] index.IndexCollection (IndexCollection.java:638) - # Final Counter Values\n",
      "2019-05-08 12:45:00,930 INFO  [main] index.IndexCollection (IndexCollection.java:639) - indexed:                5\n",
      "2019-05-08 12:45:00,930 INFO  [main] index.IndexCollection (IndexCollection.java:640) - empty:                  0\n",
      "2019-05-08 12:45:00,930 INFO  [main] index.IndexCollection (IndexCollection.java:641) - unindexed:              0\n",
      "2019-05-08 12:45:00,931 INFO  [main] index.IndexCollection (IndexCollection.java:642) - unindexable:            0\n",
      "2019-05-08 12:45:00,931 INFO  [main] index.IndexCollection (IndexCollection.java:643) - skipped:                0\n",
      "2019-05-08 12:45:00,931 INFO  [main] index.IndexCollection (IndexCollection.java:644) - errors:                 0\n",
      "2019-05-08 12:45:00,939 INFO  [main] index.IndexCollection (IndexCollection.java:647) - Total 5 documents indexed in 00:00:00\n",
      "\n",
      "('Index error: ', None)\n",
      "./workdir/122039_temp/trec_query_file\n",
      "['sh', '../anserini/target/appassembler/bin/SearchCollection', '-topicreader', 'Trec', '-index', './workdir/122039_temp/index/', '-topics', './workdir/122039_temp/trec_query_file', '-output', './workdir/122039_temp/retrieved_doc_file', '-bm25', '-b', '0.3', '-k1', '3.7', '-rm3', '-rm3.fbDocs', '159.0', '-rm3.fbTerms', '459.0', '-rm3.originalQueryWeight', '0.7', '-hits', '1', '-threads', '10']\n",
      "('Searching error: ', None)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_file = './data/tvqa_train_processed.json'\n",
    "    val_file = './data/tvqa_val_processed.json'\n",
    "    build_index_flag = 'yes'\n",
    "    random_search = 'yes'\n",
    "    \n",
    "    hits = 1\n",
    "    \n",
    "    anserini_loc = '../anserini/'\n",
    "    n_rand_iter = 5\n",
    "    pool_size = 1\n",
    "    \n",
    "    with open(val_file, 'r') as f:\n",
    "        processed_data_val = json.load(f)\n",
    "\n",
    "    with open(train_file, 'r') as f:\n",
    "        processed_data_train = json.load(f)\n",
    "        \n",
    "    # Options\n",
    "    mode = 'val'\n",
    "    model_type = 'qa'\n",
    "#     model_type = 'sa'\n",
    "#     model_type = 'retrieval'\n",
    "    \n",
    "    workdir = './workdir/'\n",
    "    if not os.path.exists(workdir):\n",
    "        os.makedirs(workdir)\n",
    "    \n",
    "    if mode == 'val':\n",
    "        print('Validation mode: ')\n",
    "        questions_data = processed_data_val[0:100]\n",
    "    elif mode == 'test':\n",
    "        questions_data = processed_data_train\n",
    " \n",
    "\n",
    "    best_model_params_file = './baselines/best_ir_model/tvqa_bm25_rm3_best_model_dev.json'\n",
    "    \n",
    "    find_best_dev_model(best_model_params_file, n_rand_iter, pool_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [predicted_answers, gold_answers] = retrieval_model(processed_data_val, processed_data_train)\n",
    "# preds = np.asarray(predicted_answers)\n",
    "# targets = np.asarray(gold_answers)\n",
    "# acc = sum(preds == targets) / float(len(preds))\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_train[3000]['answer_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train = [ele['q'] for ele in processed_data_train]\n",
    "questions_val = [ele['q'] for ele in processed_data_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TfidfVectorizer()\n",
    "td_2 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_q_train = td.fit_transform(questions_train)\n",
    "tfidf_q_val = td_2.fit_transform(questions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_q_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_data_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
